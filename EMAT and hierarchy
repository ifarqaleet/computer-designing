There are two memory hierarchies, 

Hierarchal access, sequential access. 

In hierarchal we access one after another 
In sequential we access parallely 

In sequential we take less amount of time. 




-----
Cache inconsistency issues: 

Lets say data a=1 is being called again and again and hence it has many copies throughout the heirarchy. There is teh same replicated data in main, 
cache, secondary etc. In this situation if the value of a in cache changes (since cache is closest to the cpu) there is a need to change the value in 
main and other memories as well so that consistency is maintained. 

To maintain this, we have two concepts 
1. Write back 
2. Write through 

Write back means, making all the changes in the cache and in the end once we are done, updating it in the main. This is the best policy as it reducing 
number of main memory access ( remember that main mem acccess take a loooot of time) 

Write through basically means that as soon as the value in the cache changes, we must reflect the same change immediately in the main as well. This will lead to 
the number of main memory access 


ðŸ”¹ Cache Inclusion
This describes the relationship between multiple levels of cache (L1, L2, L3).
Inclusive cache:
Everything in L1 is also guaranteed to be in L2 (and so on).
Advantage: Easy to maintain coherence.
Disadvantage: Wastes space because data is duplicated.
Exclusive cache:
A block exists in only one level at a time.
Advantage: Uses cache space more efficiently (no duplication).
Disadvantage: More complex management, harder to maintain coherence.
Non-inclusive (or non-exclusive):
No strict rule; blocks may or may not be present in multiple levels.
Many modern CPUs use this approach for flexibility.


Cache hierarchy. 

In cache there are two different types of parallel caches. D and I Cache. 
D - for operands
I - For instructions and data 

these two are a part of L1 cache. 

They work in parallel 


-----
CONCEPT: 


Ah â€” that's the key point of confusion. Let's think about the hierarchy:

---

**1. Cache block sizes**

- **L1 cache block** = 4 words  
- **L2 cache block** = 16 words  

When there's a miss in L1, the CPU needs **one L1 block** (4 words) from the next level (L2).  
But if L2 also misses, L2 must fetch **one L2 block** (16 words) from main memory.

---

**2. Why fetch 16 words if L1 only needs 4?**

Because caches work on the principle of **spatial locality** and manage data in fixed-size blocks.

- L2 is not just a "pass-through" â€” it stores data in blocks of 16 words.  
- When L2 gets a miss, it must bring the **entire L2 block** (16 words) from main memory into the L2 cache.  
- Then, out of that 16-word block in L2, the requested 4-word block for L1 is transferred to L1.

So even though the CPU only needs 4 words right now, the memory system fetches a larger block (16 words) into L2 to **reduce future misses** â€” if the CPU accesses nearby words soon, they will already be in L2.

---

**3. Analogy**

Imagine you have a bookshelf (L1) that holds one book (4 words).  
A larger cabinet (L2) holds 4 books in a box (16 words).  
If you want a book that's not on the shelf, you check the cabinet. If it's not in the cabinet either, you get the **entire box** (16 words) from the warehouse (main memory) and put it in the cabinet. Then you take the one book you need from the box and put it on the shelf.

---

**4. Summary**

So the 16-word transfer from memory to L2 is because **L2's block size is 16 words**, not because L1 needs 16 words.  
L1 will only get 4 words from that 16-word block in L2.

Thatâ€™s why we transfer 16 words (in 4 chunks of 4 words each) from memory to L2, then 4 words from L2 to L1.
